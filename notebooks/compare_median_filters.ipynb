{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71948fa1",
   "metadata": {},
   "source": [
    "# Compare original median filter with normalized median filter.\n",
    "Once a velocity field has been obtained from a PIV processor, there might be vectors that, visually, look as outliers. One way to get rid of them is to apply a median filter: compare every velocity vector to the average vector in its, say, 3x3 neighborhoud and, if it is bigger than a threshold, replace it with the average vector.\n",
    "\n",
    "As described in Adrain & Westerweel, \"Particle image velocimetry\", 2011, J. Westerweel and coauthors created two versions of the median filter. The original one was created in 1994. Its main disadvantage is that it uses one (global) threshold for all the vectors in the velocity field. In 2005, it was proposed to normalize every vector in the velocity field before comparing it to the global threshold, thereby mitigating the disadvantage of the 1994 version of the median filter (see J. Westerweel, F. Scarano, \"Universal outlier detection for PIV data\", Experiments in fluids, 39(6), p.1096-1100, 2005).\n",
    "\n",
    "OpenPIV has implemented both version of the filter. The 1994 version (the original version) is given by the function `validation.local_median_val()`. The 2005 version (the normalized version) is given by the function `validation.local_norm_median_val()`.\n",
    "\n",
    "The phylosophy of their usage is the following. Both filters just check every vector in the velocity field and create a \"mask\" of the velocity field where those vector that didn't pass the threshold requirement are marked as NaNs. Then the OpenPIV function `filters.replace_outliers()` must be implemented. That function reads the \"mask\" and replace every NaN vector with the average of its neighbourhood.\n",
    "\n",
    "The purpose of this tutorial is compare the two fitlers for a rather difficult case of PIV of a two-phase cap_bubbly air-water flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b98dbef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m \u001b[38;5;66;03m# conda install opencv; alternatively use imread() from openpiv tools\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import cv2 # conda install opencv; alternatively use imread() from openpiv tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openpiv import tools, pyprocess, preprocess, validation, filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9c768",
   "metadata": {},
   "source": [
    "Now let's write a function that does basic PIV processing on a pair of PIV frames. The function will find calculate correlations with `pyprocess.fft_correlate_images()`, then it will convert correlations to displacements with `pyprocess.correlation_to_displacement` and, finally, it will validate the velocity field and replace the outliers. It is very imporatant to understand what the right strategy is for the validation and replacement of the outliers. There are two types of validation: validation of the correlations and validation of the velocity field. Validation of the correlations makes sure that the largest correlation peak is much bigger than the rest of the peaks, thereby making sure that the largest correlation peak is not just noice but a valid signal. Once the correlations have been validated this way, one goes ahead and calculates the velocity field. Surprisingly, one can, still, obtain a velocity field with outliers (some vectors are much bigger or point in the direction different than the vectors surrounding it). I.e., correlations validation doesn't give a 100% warranty that the resultant velocity field will be absolutely accurate. So, one goes ahead and validates the velocity field with a median filter which compares every vector to several of its surrounding vectors. Both validation types just mark the invalid vectors. Then the replacement function goes through all the marked vectors and replaces every one of them with the average of its surrounding vectors. Now imagine if the majority of the vectors have been marked as invalid. In this case, the chance of getting invalid vectors in the surroundings of other invalid vectors is big. And we end up with completely \"remodeled\" velocity field where almost every vector has been replaced with the average of its surroundings. I.e., the resultant field is not physical. Obviously, if you choose to do both validations types (which, very well, can be the case), then you have invalid vectors due to correlations and due to outliers. Of course, some of them may be the same, but some of them may not. I.e., you increasing the number of invalid vectors for further replacement procedure. You want to avoid it. In order to avoid it, do validation-replacement in steps. First, validate correlations and replace them at once. Second, validate outliers and replace them at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIVanalysis(dataArray, \n",
    "                imageShape, \n",
    "                searchAreaSize,\n",
    "                overlap,\n",
    "                scalingFactor,\n",
    "                deltaT,\n",
    "                maskBubble,\n",
    "                nameTXT,\n",
    "                medianTest,\n",
    "                onImg = None,\n",
    "                s2nThresh = 0.8                \n",
    "               ):\n",
    "    \n",
    "    \"\"\"\n",
    "    The procedure is adapted from ensemble_correlation.ipynb OpenPIV tutorial.\n",
    "    Parameters: dataArray (numpy array) - an array of PIV images that contains only\n",
    "                                          one PIV pair, its structure is\n",
    "                                          dataArray = np.asarray([frameA, frameB])\n",
    "                onImg (cv2.imread) - an image to plot the velocity fields on \n",
    "                searchAreaSize (int) - is the size of the interrogation window on frameB;\n",
    "                                       it can be a bit bigger, which, reportedly, is better\n",
    "                                       for PIV pair with high dynamic range (like in our\n",
    "                                       case, where velocities in the bubble wake are much\n",
    "                                       bigger than velocities closer to the walls of the test section -\n",
    "                                       the range of velocities (i.e., dynamic range) is big)\n",
    "                maskBubble (numpy array) - a binary image where bubbles are white and background is\n",
    "                                           black; is used to mask out bubbles\n",
    "                deltaT (float) - s, the time between the two PIV frames\n",
    "                scalingFactor (float) - pix/mm, scaling factor\n",
    "                nameTXT (str) - .txt, the name of the file to save the resultant OpenPIV velocity field\n",
    "                medianTest (int) - can be either 1994 or 2005; 1994 refers to original Westerweel's\n",
    "                                   median test given by the function validation.local_median_val(),\n",
    "                                   2005 refers to the normalized Westerweel's median test given by the\n",
    "                                   function validation.local_norm_median_val().\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Mask out the bubbles.\n",
    "    dataArray[0] = cv2.bitwise_and(dataArray[0], dataArray[0], mask=cv2.bitwise_not(maskT))\n",
    "    dataArray[1] = cv2.bitwise_and(dataArray[1], dataArray[1], mask=cv2.bitwise_not(maskT))\n",
    "\n",
    "    # Find the correlations map.\n",
    "    # \"linear\" correlation_method together with normalized_correlation=True\n",
    "    # helps to boost the s2n threshold from 1.003 to 2 for 95% valid vectors.\n",
    "    corrs = pyprocess.fft_correlate_images(\n",
    "        pyprocess.moving_window_array(dataArray[0],searchAreaSize,overlap),\n",
    "        pyprocess.moving_window_array(dataArray[1],searchAreaSize,overlap),\n",
    "        normalized_correlation=True,\n",
    "        correlation_method=\"linear\"\n",
    "    )\n",
    "    \n",
    "    mesh = pyprocess.get_field_shape(imageShape,\n",
    "                                     search_area_size = searchAreaSize,\n",
    "                                     overlap = overlap\n",
    "    )\n",
    "\n",
    "    nrows, ncols = mesh[0], mesh[1]\n",
    "    \n",
    "    # Having found correlations, we can find corresponding displacements in pix.\n",
    "    # IMPOTANT: OpenPIV uses u and v for displacements as well as for velocities in\n",
    "    # Since that's confusing, we are going to use xDisp and yDisp for \n",
    "    # displacements and u and v for velocities.\n",
    "    xDisp, yDisp = pyprocess.correlation_to_displacement(corrs, nrows, ncols)\n",
    "\n",
    "    # Having found displacements in pix, we can find corresponding velocities in\n",
    "    # pix/time\n",
    "    u = xDisp/deltaT # pix/s\n",
    "    v = yDisp/deltaT # pix/s\n",
    "\n",
    "    x, y = pyprocess.get_coordinates(imageShape,\n",
    "                                     search_area_size = searchAreaSize,\n",
    "                                     overlap = overlap\n",
    "    )\n",
    "\n",
    "    # Mask out the areas without the flow right now (bubbles).\n",
    "    grid_mask = preprocess.prepare_mask_on_grid(x, y, maskT[:,::-1])\n",
    "    masked_u = np.ma.masked_array(u, mask=grid_mask)\n",
    "    masked_v = np.ma.masked_array(v, mask=grid_mask)\n",
    "\n",
    "    # Now let's do validation. Pay attention, that validation doesn't mean automatic\n",
    "    # replacement of the outliers, it means that we are going to detect the outliers\n",
    "    # and mark them as outliers. Their replacement is a separate procedure and will\n",
    "    # be done next.\n",
    "\n",
    "    # Do the validation of the correlation peaks.\n",
    "    corrs = corrs.astype('float64')\n",
    "    s2n = pyprocess.sig2noise_ratio(corrs, \"peak2mean\")\n",
    "    invalid_mask_s2n = validation.sig2noise_val(s2n, threshold = threshold)\n",
    "    # Now we can replace outliers flagged by the invalid_mask_s2n.\n",
    "    masked_u, masked_v = filters.replace_outliers(masked_u.flatten(), masked_v.flatten(),\n",
    "                                                  invalid_mask_s2n,\n",
    "                                                  method = 'localmean',\n",
    "                                                  max_iter = 3, \n",
    "                                                  kernel_size = 17 \n",
    "    ) # IMPORTANT to flatten u and v\n",
    "    \n",
    "    # Do the validation based on the median test (as described in the \n",
    "    # German PIV book on p.185). We can do it in two ways. First - use\n",
    "    # the original Westerweel's median test as implemented in OpenPIV's \n",
    "    # function validation.local_median_val. Second - use the improved (normalized)\n",
    "    # Westerweel's median test which is implemented in OpenPIV's function\n",
    "    # validation.local_norm_median_val.\n",
    "    # The problem is that  filters.replace_outliers() returned flattened arrays on\n",
    "    # the previous step. But both validation.local_median_val() and \n",
    "    # validation.local_norm_median_val() require 2D arrays as the input. The way to\n",
    "    # transfrom the flattened arrays to the 2D arrays is the following:\n",
    "    uShape = masked_u.shape\n",
    "    masked_u = np.reshape(masked_u,(uShape[0],uShape[1])) # a 2D array\n",
    "    masked_v = np.reshape(masked_v,(uShape[0],uShape[1])) # a 2D array\n",
    "    if medianTest == 1994:\n",
    "        # ORIGINAL WESTERWEEL'S MEDIAN TEST.\n",
    "        invalid_mask_median = validation.local_median_val(masked_u, \n",
    "                                                          masked_v,\n",
    "                                                          u_threshold=4000, # abs difference with the mean\n",
    "                                                          v_threshold=4000, # abs difference with the mean\n",
    "                                                          size=8 # e.g., size=2 is a 5x5 kernel\n",
    "        )\n",
    "    elif medianTest == 2005:\n",
    "        # NORMALIZED WESTERWEEL'S MEDIAN TEST.\n",
    "        # One doesn't have to restrict oneself just to one instance of the median test.\n",
    "        # For this particular dataset, after an extensive test of different parameters, it\n",
    "        # was concluded that two median tests are necessary. The reason for that was the following.\n",
    "        # The velocity field had a lot of very obvious outliers close to each other. Yet it also\n",
    "        # had less obvious outliers sitting far away from each other. The first median filter uses a\n",
    "        # huge kernel size which is bigger than the width of the set of the neighbouring outliers.\n",
    "        # I.e., if I have 5 outliers in a row, I want my kernels size to be bigger than 5 because, in \n",
    "        # this case, valid vectors will be included into the averaging procedure. If the kernel is less\n",
    "        # than 5, it will include only the outliers into the averaging procedure resulting in no good.\n",
    "        # I.e., we will be compare the outlier to the average of the outliers around it, therefore we\n",
    "        # will not detect it as the outlier because, obviously, it will be on the order of the average of\n",
    "        # the outliers.\n",
    "        # At the second step, when we are left with only less obvious outliers which are sittig far away\n",
    "        # from each other, we're doing median filter with the smallest kernel size possible. Since the \n",
    "        # outliers are less obvious, they can be barealy visually distinguishable from other vectors. \n",
    "        # That is why the kernel size must be so small: it compares the outlier only to its immediate \n",
    "        # neighbours and detects it as an outlier. If it compared the outlier to more vectors, the chance\n",
    "        # that more vectors make the average of them bigger is high. And since we compare the outlier to\n",
    "        # the average of the surrounding vectors, we might not detect it as an outlier. That's why we\n",
    "        # we want the smallest kernel size in this case.\n",
    "\n",
    "        # Do the first run of the median test. \n",
    "        invalid_mask_median = validation.local_norm_median_val(masked_u,\n",
    "                                                               masked_v,\n",
    "                                                               ε = 0.2,\n",
    "                                                               threshold = 1.0,\n",
    "                                                               size = 17\n",
    "        )\n",
    "        # Now we can replace outliers flagged by the invalid_mask_median1.        \n",
    "        masked_u, masked_v = filters.replace_outliers(masked_u.flatten(), masked_v.flatten(),\n",
    "                                                      invalid_mask_median1.flatten(),\n",
    "                                                      method = 'localmean',\n",
    "                                                      max_iter = 3, \n",
    "                                                      kernel_size = 17  \n",
    "        ) # IMPORTANT to flatten u and v\n",
    "\n",
    "        # Do the second run of the median test.\n",
    "        # Start with reshaping masked_u and masked_v to 2D arrays.\n",
    "        masked_u = np.reshape(masked_u,(uShape[0],uShape[1])) # a 2D array\n",
    "        masked_v = np.reshape(masked_v,(uShape[0],uShape[1])) # a 2D array\n",
    "        invalid_mask_median2 = validation.local_norm_median_val(masked_u,\n",
    "                                                                masked_v,\n",
    "                                                                ε = 0.2,\n",
    "                                                                threshold = 1.0,\n",
    "                                                                size = 1\n",
    "        )\n",
    "        masked_u, masked_v = filters.replace_outliers(masked_u.flatten(), masked_v.flatten(),\n",
    "                                                      invalid_mask_median2.flatten(),\n",
    "                                                      method = 'localmean',\n",
    "                                                      max_iter = 3, \n",
    "                                                      kernel_size = 1 \n",
    "        ) # IMPORTANT to flatten u and v\n",
    "\n",
    "        # When we will be saving the results into the OpenPIV .txt file, we will want to\n",
    "        # know about all the invalid vectors. That's why we are combining the masks.\n",
    "        # The way they are combined is copied from the code for OpenPIV.validation.typical_validation\n",
    "        # line 295. These are, actually, flags, not masks.\n",
    "        invalid_mask_median = invalid_mask_median1 | invalid_mask_median2\n",
    "\n",
    "    # When we will be saving the results into the OpenPIV .txt file, we will want to\n",
    "    # know about all the invalid vectors. That's why we are combining the masks.\n",
    "    # The way they are combined is copied from the code for OpenPIV.validation.typical_validation\n",
    "    # line 295. These are, actually, flags, not masks.\n",
    "    invalid_mask = invalid_mask_s2n | invalid_mask_median.flatten()\n",
    "\n",
    "    x, y, masked_u, masked_v = scaling.uniform(x, y, masked_u, masked_v, scaling_factor = scalingFactor)\n",
    "\n",
    "    # MASK OUT VELOCITY FIELD.\n",
    "    # Before saving the field to a .txt file, give zeros to those vectors that lie in the masked regions.\n",
    "    # Right now, x and y are in the image system of coordinates: x is to the right, y is downwards, (0,0)\n",
    "    # is in the top left corner. It can be learnt that from the GitHub code of tools.transform_coordinates.\n",
    "    # Since our x and y are in mm, we're going to use scaling factor to convert them to pix. Then we're going\n",
    "    # to use them to identify whether or not their place on an example masked image is masked.\n",
    "    xFlat = x.flatten()\n",
    "    yFlat = y.flatten()\n",
    "    for i in range(xFlat.size):\n",
    "        if maskT[int(yFlat[i]*scalingFactor), int(xFlat[i]*scalingFactor)] == 255:\n",
    "                masked_u[i] = 0\n",
    "                masked_v[i] = 0\n",
    "    \n",
    "    x, y, masked_u, masked_v = tools.transform_coordinates(x, y, masked_u, masked_v)\n",
    "\n",
    "    tools.save(nameTXT, x, y, masked_u, masked_v, invalid_mask)\n",
    "    \n",
    "    # Calculate the percentage of the invalid vectors due to median validation.\n",
    "    invMedOutPer = (np.count_nonzero(invalid_mask_median) / invalid_mask_median.size) * 100 # percent of the invalid vectors\n",
    "    # Calculate the percentage of the invalid vectors due to sig2noise validation.\n",
    "    invSnrOutPer = (np.count_nonzero(invalid_mask_s2n) / invalid_mask_s2n.size) * 100 # percent of the invalid vectors\n",
    "    # Calculate the percentage of the invalid vectors due to both median and sig2noise validations.\n",
    "    invOutPer = (np.count_nonzero(invalid_mask) / invalid_mask.size) * 100 # percent of the invalid vectors\n",
    "    print(\"\\nPercentage of invalid vectors: invalidMedianOutliersPercent={}, invalidS2NOutliersPercent={}, invalitOutliersPercent={}\".format(invMedOutPer,invSnrOutPer,invOutPer))\n",
    "    figure, _ = tools.display_vector_field(filename = str(nameTXT),\n",
    "                                           scaling_factor = scalingFactor,\n",
    "                                           scale = 5000,\n",
    "                                           on_img = True,\n",
    "                                           show_invalid = True,\n",
    "                                           image_name = onImg\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fbe300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "tPulse = 0.0015 # s - time between frame A and frame B\n",
    "ol = 16 # overlap: give it in pix, not %\n",
    "interW = 32 # the size of the interrogation window\n",
    "searchAS = np.round(1.1*interW) # the size of the interrogation window in frameB \n",
    "sf = 1/0.052 # pix/mm scaling factor\n",
    "\n",
    "# The name of the OpenPIV txt file to save the results to.\n",
    "fileTXT = '../test21/bubblesOpenPIVtxtFile.txt'\n",
    "\n",
    "# The file names of PIV frames and bubbles mask.\n",
    "frameA = '../test21/bubblesFrameA.png'\n",
    "frameB = '../test21/bubblesFrameB.png'\n",
    "bubblesMaskFile = '../test21/bubblesMask.png'\n",
    "\n",
    "pairPIV = np.asarray([cv2.imread(frameA,0), cv2.imread(frameB,0)]) # 0 flag means read as gray scale\n",
    "bubblesMask = cv2.imread(bubblesMaskFile,0) # 0 flag means read as gray scale\n",
    "imageShape = pairPIV[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIVanalysis(dataArray = pairPIV, \n",
    "            imageShape = imageShape, \n",
    "            searchAreaSize = searchAS,\n",
    "            overlap = ol,\n",
    "            scalingFactor = sf,\n",
    "            deltaT = tPulse,\n",
    "            maskBubble = bubblesMask,\n",
    "            nameTXT = fileTXT,\n",
    "            medianTest = 1994,\n",
    "            onImg = True,\n",
    "            s2nThresh = 2, \n",
    "            onImg = bubblesMaskFile                 \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.878747,
   "end_time": "2022-10-20T19:15:56.638287",
   "environment_variables": {},
   "exception": true,
   "input_path": "./notebooks/analyse_movie.ipynb",
   "output_path": "./notebooks/analyse_movie.ipynb",
   "parameters": {},
   "start_time": "2022-10-20T19:15:53.759540",
   "version": "2.4.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
